{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ed20fe33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import spacy \n",
    "import optuna\n",
    "import umap\n",
    "import xgboost as xgb\n",
    "import re\n",
    "\n",
    "import sys; sys.path.insert(0, '../') #adds all the code we've written in src\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from preprocessing import *\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearnex.svm import SVC\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer as tfidf\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import warnings #this my friends is overconfidence at it's finest\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f91b27",
   "metadata": {},
   "source": [
    "# Encoding Data and Creating Train, Validation, Testing Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de1e8f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of raw data\n",
      "\n",
      "X: (41176,)\n",
      "Y: (41176,)\n",
      "\n",
      "Shape of split data\n",
      "\n",
      "Train: (28839,)\n",
      "Validation: (6160,)\n",
      "Test: (6177,)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayush\\.conda\\envs\\nlp\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of BOW \n",
      "\n",
      "Train: (28839, 38255)\n",
      "Validation: (6160, 38255)\n",
      "Test: (6177, 38255)\n",
      "\n",
      "Shape of tf-idf \n",
      "\n",
      "Train: (28839, 38255)\n",
      "Validation: (6160, 38255)\n",
      "Test: (6177, 38255)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../cyberbullying_tweets/processed_tweets.csv\", index_col = 0)\n",
    "X = df.ascii.to_numpy()\n",
    "y = df.y.to_numpy()\n",
    "\n",
    "print(f\"Shape of raw data\\n\\nX: {X.shape}\\nY: {y.shape}\\n\")\n",
    "\n",
    "###create train, test, and validation splits of the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42122, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.176, random_state=42122, stratify=y_train)\n",
    "\n",
    "print(f\"Shape of split data\\n\\nTrain: {X_train.shape}\\nValidation: {X_val.shape}\\nTest: {X_test.shape}\\n\")\n",
    "\n",
    "###fit bag-of-words embeddings on training data and then use to transform all splits \n",
    "bow_vectorizer = CountVectorizer(tokenizer=word_tokenize, analyzer = 'word', stop_words=None, ngram_range=(1,1), lowercase=False)\n",
    "bow_vectorizer.fit(X_train)\n",
    "\n",
    "X_train_bow, X_val_bow, X_test_bow = (bow_vectorizer.transform(dataset) for dataset in [X_train, X_val, X_test])\n",
    "\n",
    "print(f\"Shape of BOW \\n\\nTrain: {X_train_bow.shape}\\nValidation: {X_val_bow.shape}\\nTest: {X_test_bow.shape}\\n\")\n",
    "\n",
    "###fit tf-idf embeddings on training data and then use to transform all splits\n",
    "tf_vectorizer = tfidf()\n",
    "tf_vectorizer.fit(X_train_bow)\n",
    "\n",
    "X_train_tfidf, X_val_tfidf, X_test_tfidf = (tf_vectorizer.transform(dataset) for dataset in [X_train_bow, X_val_bow, X_test_bow])\n",
    "\n",
    "print(f\"Shape of tf-idf \\n\\nTrain: {X_train_bow.shape}\\nValidation: {X_val_bow.shape}\\nTest: {X_test_bow.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3512a69",
   "metadata": {},
   "source": [
    "# Logistic Regression \n",
    "\n",
    "The multinomial logistic regression didn't require any complicated hyperparamter tuning. The multinomial setting did well in testing, and excessive normalization didn't make too much of a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0429b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW Embedding Classification Report\n",
      "Model Accuracy: 0.8526792941557391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.63      0.61       899\n",
      "           1       0.90      0.87      0.88      1083\n",
      "           2       0.95      0.94      0.94      1183\n",
      "           3       0.60      0.61      0.60       744\n",
      "           4       0.97      0.97      0.97      1177\n",
      "           5       0.99      0.97      0.98      1091\n",
      "\n",
      "    accuracy                           0.85      6177\n",
      "   macro avg       0.83      0.83      0.83      6177\n",
      "weighted avg       0.86      0.85      0.85      6177\n",
      " \n",
      "\n",
      "TFIDF Embedding Classification Report\n",
      "Model Accuracy: 0.8473368949328153\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.61      0.59       899\n",
      "           1       0.90      0.85      0.88      1083\n",
      "           2       0.93      0.95      0.94      1183\n",
      "           3       0.62      0.59      0.60       744\n",
      "           4       0.95      0.97      0.96      1177\n",
      "           5       0.98      0.97      0.97      1091\n",
      "\n",
      "    accuracy                           0.85      6177\n",
      "   macro avg       0.83      0.82      0.82      6177\n",
      "weighted avg       0.85      0.85      0.85      6177\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg_bow = LR(max_iter=10000, solver = 'saga', multi_class='multinomial').fit(X_train_bow, y_train)\n",
    "y_pred_lr_bow = log_reg_bow.predict(X_test_bow)\n",
    "\n",
    "print(\"BOW Embedding Classification Report\")\n",
    "print(f\"Model Accuracy: {accuracy(y_test, y_pred_lr_bow)}\")\n",
    "print(classification_report(y_test, y_pred_lr_bow), \"\\n\")\n",
    "\n",
    "log_reg_tfidf = LR(max_iter=10000, solver = 'saga', multi_class='multinomial').fit(X_train_tfidf, y_train)\n",
    "y_pred_lr_tfidf = log_reg_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"TFIDF Embedding Classification Report\")\n",
    "print(f\"Model Accuracy: {accuracy(y_test, y_pred_lr_tfidf)}\")\n",
    "print(classification_report(y_test, y_pred_lr_tfidf), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3e4e0",
   "metadata": {},
   "source": [
    "# SVMs and Optuna \n",
    "\n",
    "Here we create the support vector machine and train it's hyperparamters using Optuna. Optuna relies on bayesian hyperparamter sampling rather than a reductive grid search. This saves computation time while ensuring good results. The trial defines the search space for hyperparamters. This process must be repeated for both BOW and TF-IDF embeddings. The trial will attempt to maximize the validation accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f08ce784",
   "metadata": {},
   "outputs": [],
   "source": [
    "class svm_optimizer():\n",
    "\n",
    "    def __init__(self, X_train, X_val, X_test, y_train, y_val, y_test, study_name): \n",
    "        \"\"\"\n",
    "        Initialize Objectives with data. Allows for code reuse. \n",
    "        \"\"\"\n",
    "        self.models = [] #save all models tested\n",
    "        self.X_train, self.X_val, self.X_test = X_train, X_val, X_test\n",
    "        self.y_train, self.y_val, self.y_test = y_train, y_val, y_test\n",
    "        self.study = optuna.create_study(study_name = study_name, direction = \"maximize\")\n",
    "    \n",
    "    def objective(self, trial): \n",
    "        kernels = [\"linear\", \"rbf\", \"sigmoid\"]\n",
    "        kernel = trial.suggest_categorical(\"kernel\", kernels)\n",
    "        regularization = trial.suggest_float(\"regularization\", 0, 1)\n",
    "        gamma = trial.suggest_loguniform(\"gamma\", 10e-3, 10e3)\n",
    "\n",
    "        model = SVC(max_iter = 10000, C = regularization, gamma = gamma,\n",
    "                    kernel = kernel).fit(self.X_train, self.y_train)\n",
    "        self.models.append(model) \n",
    "\n",
    "        y_pred = model.predict(self.X_val)\n",
    "        return accuracy(self.y_val, y_pred)\n",
    "    \n",
    "    def optimize(self, n_trials = 20):\n",
    "        \"\"\"\n",
    "        Optimizes the objective with the given study. Can be called multiple times \n",
    "        and will save the state. \n",
    "        \"\"\"\n",
    "        self.study.optimize(self.objective, n_trials = n_trials)\n",
    "        \n",
    "    def get_best_model(self):\n",
    "        \"\"\"\n",
    "        Returns None if no optimization has occured. Else returns the best model \n",
    "        so far according to the study. \n",
    "        \"\"\"\n",
    "        if self.models is None: \n",
    "            return None\n",
    "        \n",
    "        return self.models[self.study.best_trial.number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f577e988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-21 23:04:46,009]\u001b[0m A new study created in memory with name: SVM BOW Optimzer\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:05:05,178]\u001b[0m Trial 0 finished with value: 0.7913961038961039 and parameters: {'kernel': 'linear', 'regularization': 0.0016986325574974703, 'gamma': 0.9686189355821254}. Best is trial 0 with value: 0.7913961038961039.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:05:30,295]\u001b[0m Trial 1 finished with value: 0.2297077922077922 and parameters: {'kernel': 'sigmoid', 'regularization': 0.6768845597479997, 'gamma': 12.703604099600875}. Best is trial 0 with value: 0.7913961038961039.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:05:55,472]\u001b[0m Trial 2 finished with value: 0.2297077922077922 and parameters: {'kernel': 'sigmoid', 'regularization': 0.5304538899902461, 'gamma': 77.50452557518204}. Best is trial 0 with value: 0.7913961038961039.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:06:10,737]\u001b[0m Trial 3 finished with value: 0.8425324675324676 and parameters: {'kernel': 'linear', 'regularization': 0.011974143645818724, 'gamma': 4.570742277697089}. Best is trial 3 with value: 0.8425324675324676.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:06:33,615]\u001b[0m Trial 4 finished with value: 0.8293831168831168 and parameters: {'kernel': 'sigmoid', 'regularization': 0.9294778344997356, 'gamma': 0.017980552324847365}. Best is trial 3 with value: 0.8425324675324676.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:07:00,142]\u001b[0m Trial 5 finished with value: 0.8461038961038961 and parameters: {'kernel': 'linear', 'regularization': 0.9833905252272902, 'gamma': 2803.0040817408794}. Best is trial 5 with value: 0.8461038961038961.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:07:17,604]\u001b[0m Trial 6 finished with value: 0.8566558441558442 and parameters: {'kernel': 'linear', 'regularization': 0.3029130293028748, 'gamma': 0.012066781287838347}. Best is trial 6 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:07:51,829]\u001b[0m Trial 7 finished with value: 0.42272727272727273 and parameters: {'kernel': 'rbf', 'regularization': 0.9547000922806371, 'gamma': 31.830053440013387}. Best is trial 6 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:08:16,413]\u001b[0m Trial 8 finished with value: 0.8456168831168831 and parameters: {'kernel': 'linear', 'regularization': 0.8994263739988161, 'gamma': 0.01603036876519055}. Best is trial 6 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:08:41,493]\u001b[0m Trial 9 finished with value: 0.22987012987012986 and parameters: {'kernel': 'sigmoid', 'regularization': 0.5062694542204517, 'gamma': 2692.7949050201523}. Best is trial 6 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:09:31,180]\u001b[0m Trial 10 finished with value: 0.3548701298701299 and parameters: {'kernel': 'rbf', 'regularization': 0.2350711526588449, 'gamma': 0.21616090479560682}. Best is trial 6 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:09:48,187]\u001b[0m Trial 11 finished with value: 0.8564935064935065 and parameters: {'kernel': 'linear', 'regularization': 0.27610658137668204, 'gamma': 9820.309828439966}. Best is trial 6 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:10:05,020]\u001b[0m Trial 12 finished with value: 0.8564935064935065 and parameters: {'kernel': 'linear', 'regularization': 0.27917791074256576, 'gamma': 241.05961922444956}. Best is trial 6 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:10:21,783]\u001b[0m Trial 13 finished with value: 0.8566558441558442 and parameters: {'kernel': 'linear', 'regularization': 0.29975031591983137, 'gamma': 0.1708633060495962}. Best is trial 6 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:10:40,473]\u001b[0m Trial 14 finished with value: 0.8551948051948052 and parameters: {'kernel': 'linear', 'regularization': 0.38525531516374456, 'gamma': 0.10260380769012546}. Best is trial 6 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:10:55,534]\u001b[0m Trial 15 finished with value: 0.8597402597402597 and parameters: {'kernel': 'linear', 'regularization': 0.16622189503965393, 'gamma': 0.11043214261244648}. Best is trial 15 with value: 0.8597402597402597.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:11:16,741]\u001b[0m Trial 16 finished with value: 0.8001623376623377 and parameters: {'kernel': 'rbf', 'regularization': 0.1324415893431257, 'gamma': 0.011468055726207904}. Best is trial 15 with value: 0.8597402597402597.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:11:31,083]\u001b[0m Trial 17 finished with value: 0.8603896103896104 and parameters: {'kernel': 'linear', 'regularization': 0.13422383454969025, 'gamma': 0.9142692069020898}. Best is trial 17 with value: 0.8603896103896104.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:11:46,569]\u001b[0m Trial 18 finished with value: 0.8600649350649351 and parameters: {'kernel': 'linear', 'regularization': 0.12979315253177376, 'gamma': 1.561966760172255}. Best is trial 17 with value: 0.8603896103896104.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:12:31,159]\u001b[0m Trial 19 finished with value: 0.4008116883116883 and parameters: {'kernel': 'rbf', 'regularization': 0.6825363965608254, 'gamma': 2.804653329894117}. Best is trial 17 with value: 0.8603896103896104.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#BOW Optimization\n",
    "svm_bow_optimizer = svm_optimizer(X_train_bow, X_val_bow, X_test_bow, y_train, y_val, y_test, \"SVM BOW Optimzer\")\n",
    "svm_bow_optimizer.optimize(20)\n",
    "svm_bow = svm_bow_optimizer.get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ecd49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-21 23:12:37,980]\u001b[0m A new study created in memory with name: SVM TF-IDF Optimizer\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:13:03,832]\u001b[0m Trial 0 finished with value: 0.8154220779220779 and parameters: {'kernel': 'rbf', 'regularization': 0.757344767451486, 'gamma': 0.04778698667791589}. Best is trial 0 with value: 0.8154220779220779.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:13:41,155]\u001b[0m Trial 1 finished with value: 0.639448051948052 and parameters: {'kernel': 'rbf', 'regularization': 0.44213525039900203, 'gamma': 165.70985546763447}. Best is trial 0 with value: 0.8154220779220779.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:14:15,929]\u001b[0m Trial 2 finished with value: 0.23863636363636365 and parameters: {'kernel': 'rbf', 'regularization': 0.7241849418837045, 'gamma': 670.9616502109103}. Best is trial 0 with value: 0.8154220779220779.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:14:37,290]\u001b[0m Trial 3 finished with value: 0.8561688311688311 and parameters: {'kernel': 'linear', 'regularization': 0.6897147072790143, 'gamma': 0.014126985518458939}. Best is trial 3 with value: 0.8561688311688311.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:14:49,535]\u001b[0m Trial 4 finished with value: 0.8527597402597402 and parameters: {'kernel': 'linear', 'regularization': 0.425003971581026, 'gamma': 0.309266706885145}. Best is trial 3 with value: 0.8561688311688311.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:15:49,115]\u001b[0m Trial 5 finished with value: 0.19724025974025974 and parameters: {'kernel': 'rbf', 'regularization': 0.5639421197014143, 'gamma': 11.109343148965772}. Best is trial 3 with value: 0.8561688311688311.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:16:04,929]\u001b[0m Trial 6 finished with value: 0.8048701298701298 and parameters: {'kernel': 'linear', 'regularization': 0.05328099880021697, 'gamma': 1110.9771430062533}. Best is trial 3 with value: 0.8561688311688311.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:16:17,331]\u001b[0m Trial 7 finished with value: 0.8576298701298701 and parameters: {'kernel': 'linear', 'regularization': 0.7401340067597352, 'gamma': 923.9255926936582}. Best is trial 7 with value: 0.8576298701298701.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:16:44,214]\u001b[0m Trial 8 finished with value: 0.6413961038961039 and parameters: {'kernel': 'rbf', 'regularization': 0.715009960478233, 'gamma': 115.23945790322608}. Best is trial 7 with value: 0.8576298701298701.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:17:07,679]\u001b[0m Trial 9 finished with value: 0.1534090909090909 and parameters: {'kernel': 'rbf', 'regularization': 0.07039605653172976, 'gamma': 9683.26717403897}. Best is trial 7 with value: 0.8576298701298701.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:17:20,175]\u001b[0m Trial 10 finished with value: 0.824025974025974 and parameters: {'kernel': 'sigmoid', 'regularization': 0.9963732152877874, 'gamma': 3.5042864114009977}. Best is trial 7 with value: 0.8576298701298701.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:17:32,773]\u001b[0m Trial 11 finished with value: 0.8597402597402597 and parameters: {'kernel': 'linear', 'regularization': 0.9384186373383406, 'gamma': 0.019448569933758084}. Best is trial 11 with value: 0.8597402597402597.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:17:45,675]\u001b[0m Trial 12 finished with value: 0.8597402597402597 and parameters: {'kernel': 'linear', 'regularization': 0.9900123891506908, 'gamma': 0.9692927740371117}. Best is trial 11 with value: 0.8597402597402597.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:17:58,803]\u001b[0m Trial 13 finished with value: 0.8592532467532468 and parameters: {'kernel': 'linear', 'regularization': 0.9704089221619858, 'gamma': 0.27209284347479445}. Best is trial 11 with value: 0.8597402597402597.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:18:13,542]\u001b[0m Trial 14 finished with value: 0.8519480519480519 and parameters: {'kernel': 'sigmoid', 'regularization': 0.881769959282559, 'gamma': 1.5474516400242546}. Best is trial 11 with value: 0.8597402597402597.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:18:26,136]\u001b[0m Trial 15 finished with value: 0.8452922077922078 and parameters: {'kernel': 'linear', 'regularization': 0.19994153558275696, 'gamma': 0.2918024637527437}. Best is trial 11 with value: 0.8597402597402597.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:18:38,656]\u001b[0m Trial 16 finished with value: 0.8592532467532468 and parameters: {'kernel': 'linear', 'regularization': 0.8663423595142528, 'gamma': 0.015314450732369143}. Best is trial 11 with value: 0.8597402597402597.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:18:50,864]\u001b[0m Trial 17 finished with value: 0.8553571428571428 and parameters: {'kernel': 'linear', 'regularization': 0.5516326740684333, 'gamma': 0.08087066481197354}. Best is trial 11 with value: 0.8597402597402597.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:19:03,880]\u001b[0m Trial 18 finished with value: 0.6332792207792208 and parameters: {'kernel': 'sigmoid', 'regularization': 0.30820843061735215, 'gamma': 15.889718840850444}. Best is trial 11 with value: 0.8597402597402597.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:19:16,363]\u001b[0m Trial 19 finished with value: 0.8592532467532468 and parameters: {'kernel': 'linear', 'regularization': 0.8464555909163561, 'gamma': 1.6947539728394303}. Best is trial 11 with value: 0.8597402597402597.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#tf_idf optimization\n",
    "svm_tfidf_optimizer = svm_optimizer(X_train_tfidf, X_val_tfidf, X_test_tfidf, y_train, y_val, y_test, \"SVM TF-IDF Optimizer\")\n",
    "svm_tfidf_optimizer.optimize(20)\n",
    "svm_tfidf = svm_tfidf_optimizer.get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acd6e1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW Embedding Classification Report\n",
      "Model Accuracy: 0.8539744212400842\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.67      0.62       899\n",
      "           1       0.91      0.85      0.88      1083\n",
      "           2       0.95      0.94      0.94      1183\n",
      "           3       0.63      0.60      0.61       744\n",
      "           4       0.98      0.96      0.97      1177\n",
      "           5       0.99      0.97      0.98      1091\n",
      "\n",
      "    accuracy                           0.85      6177\n",
      "   macro avg       0.84      0.83      0.83      6177\n",
      "weighted avg       0.86      0.85      0.86      6177\n",
      " \n",
      "\n",
      "TFIDF Embedding Classification Report\n",
      "Model Accuracy: 0.8575360207220334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.63      0.61       899\n",
      "           1       0.91      0.87      0.89      1083\n",
      "           2       0.95      0.95      0.95      1183\n",
      "           3       0.63      0.62      0.62       744\n",
      "           4       0.96      0.97      0.97      1177\n",
      "           5       0.98      0.98      0.98      1091\n",
      "\n",
      "    accuracy                           0.86      6177\n",
      "   macro avg       0.84      0.84      0.84      6177\n",
      "weighted avg       0.86      0.86      0.86      6177\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_svm_bow = svm_bow.predict(X_test_bow)\n",
    "y_pred_svm_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"BOW Embedding Classification Report\")\n",
    "print(f\"Model Accuracy: {accuracy(y_test, y_pred_svm_bow)}\")\n",
    "print(classification_report(y_test, y_pred_svm_bow), \"\\n\")\n",
    "\n",
    "\n",
    "print(\"TFIDF Embedding Classification Report\")\n",
    "print(f\"Model Accuracy: {accuracy(y_test, y_pred_svm_tfidf)}\")\n",
    "print(classification_report(y_test, y_pred_svm_tfidf), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d3071",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees with XGBoost \n",
    "\n",
    "Gradient boosted trees are a favorite for classification tasks. The data has to be loaded in slightly strange manner, but we can still use optuna for tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fbd2b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train_bow = xgb.DMatrix(X_train_bow, label=y_train)\n",
    "D_val_bow = xgb.DMatrix(X_val_bow, label = y_val)\n",
    "D_test_bow = xgb.DMatrix(X_test_bow, label = y_test)\n",
    "\n",
    "D_train_tfidf = xgb.DMatrix(X_train_tfidf, label=y_train)\n",
    "D_val_tfidf = xgb.DMatrix(X_val_tfidf, label = y_val)\n",
    "D_test_tfidf = xgb.DMatrix(X_test_tfidf, label = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0704d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class xgb_optimizer(svm_optimizer): \n",
    "    \n",
    "    def __init__(self, train, val, test, study_name): \n",
    "        self.models = []\n",
    "        self.train = train\n",
    "        self.val = val \n",
    "        self.test = test\n",
    "        self.study = optuna.create_study(study_name = study_name, direction = \"maximize\")\n",
    "\n",
    "    \n",
    "    def objective(self, trial): \n",
    "        param = {\n",
    "            'eta': trial.suggest_loguniform(\"eta\", 10e-4, 1), \n",
    "            'max_depth': trial.suggest_int(\"max_depth\", 2, 6),  \n",
    "            'booster': trial.suggest_categorical('booster', [\"gbtree\", \"dart\"]),\n",
    "            'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "            'objective': 'multi:softprob',  \n",
    "            'eval_metric': 'mlogloss',\n",
    "            'num_class': 6} \n",
    "\n",
    "        steps = 50\n",
    "\n",
    "        model = xgb.train(param, self.train, steps)\n",
    "        self.models.append(model)\n",
    "        \n",
    "        y_pred_prob = model.predict(self.val)\n",
    "        y_pred = np.argmax(y_pred_prob, axis = 1)\n",
    "\n",
    "        return accuracy(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f7a9389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-21 23:30:24,718]\u001b[0m A new study created in memory with name: XGB BOW Optimzer\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:30:34,092]\u001b[0m Trial 0 finished with value: 0.7688311688311689 and parameters: {'eta': 0.0011833063328328703, 'max_depth': 4, 'booster': 'gbtree', 'lambda': 1.6466892397349498e-08, 'alpha': 0.0871672293961406}. Best is trial 0 with value: 0.7688311688311689.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:30:44,830]\u001b[0m Trial 1 finished with value: 0.8566558441558442 and parameters: {'eta': 0.37815622982471697, 'max_depth': 5, 'booster': 'gbtree', 'lambda': 1.5322573904727991e-06, 'alpha': 0.00033642201582319536}. Best is trial 1 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:31:36,373]\u001b[0m Trial 2 finished with value: 0.8323051948051948 and parameters: {'eta': 0.07114725886605043, 'max_depth': 6, 'booster': 'dart', 'lambda': 0.02138047734174046, 'alpha': 8.863435827800102e-08}. Best is trial 1 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:31:43,925]\u001b[0m Trial 3 finished with value: 0.7571428571428571 and parameters: {'eta': 0.006175842954112996, 'max_depth': 3, 'booster': 'gbtree', 'lambda': 0.015215764276971236, 'alpha': 1.6924860046309338e-06}. Best is trial 1 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:32:18,115]\u001b[0m Trial 4 finished with value: 0.7706168831168831 and parameters: {'eta': 0.004886476319432833, 'max_depth': 4, 'booster': 'dart', 'lambda': 0.988433058108904, 'alpha': 1.2661766242262454e-08}. Best is trial 1 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:32:46,525]\u001b[0m Trial 5 finished with value: 0.8413961038961039 and parameters: {'eta': 0.4498479766629023, 'max_depth': 2, 'booster': 'dart', 'lambda': 9.197382137506248e-05, 'alpha': 8.945465765495428e-06}. Best is trial 1 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:32:50,776]\u001b[0m Trial 6 finished with value: 0.774512987012987 and parameters: {'eta': 0.03853693035738892, 'max_depth': 2, 'booster': 'gbtree', 'lambda': 2.6726464311019572e-05, 'alpha': 9.056923936595253e-05}. Best is trial 1 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:33:21,946]\u001b[0m Trial 7 finished with value: 0.7935064935064935 and parameters: {'eta': 0.03374138991063343, 'max_depth': 3, 'booster': 'dart', 'lambda': 0.0001231967337208147, 'alpha': 0.17526201457460638}. Best is trial 1 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:33:54,959]\u001b[0m Trial 8 finished with value: 0.7933441558441559 and parameters: {'eta': 0.032603272673341184, 'max_depth': 3, 'booster': 'dart', 'lambda': 6.312372987199496e-07, 'alpha': 3.257370946490736e-05}. Best is trial 1 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:34:05,282]\u001b[0m Trial 9 finished with value: 0.8456168831168831 and parameters: {'eta': 0.18968451693716912, 'max_depth': 5, 'booster': 'gbtree', 'lambda': 0.005461472193223178, 'alpha': 5.176613010211704e-08}. Best is trial 1 with value: 0.8566558441558442.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:34:16,840]\u001b[0m Trial 10 finished with value: 0.8584415584415584 and parameters: {'eta': 0.603340994621071, 'max_depth': 6, 'booster': 'gbtree', 'lambda': 3.9256873700294286e-07, 'alpha': 0.0019550395751437165}. Best is trial 10 with value: 0.8584415584415584.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:34:28,158]\u001b[0m Trial 11 finished with value: 0.8560064935064935 and parameters: {'eta': 0.7996195316504574, 'max_depth': 6, 'booster': 'gbtree', 'lambda': 2.2741766276989794e-07, 'alpha': 0.004331408631567544}. Best is trial 10 with value: 0.8584415584415584.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:34:38,797]\u001b[0m Trial 12 finished with value: 0.849025974025974 and parameters: {'eta': 0.20996477945946215, 'max_depth': 5, 'booster': 'gbtree', 'lambda': 1.6885036431295107e-06, 'alpha': 0.004595111000547137}. Best is trial 10 with value: 0.8584415584415584.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:34:50,150]\u001b[0m Trial 13 finished with value: 0.8452922077922078 and parameters: {'eta': 0.18718781000240783, 'max_depth': 5, 'booster': 'gbtree', 'lambda': 3.366288169108082e-08, 'alpha': 0.00222549927912058}. Best is trial 10 with value: 0.8584415584415584.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:35:05,954]\u001b[0m Trial 14 finished with value: 0.8590909090909091 and parameters: {'eta': 0.9505592556873367, 'max_depth': 6, 'booster': 'gbtree', 'lambda': 4.8908657459087586e-06, 'alpha': 0.0006384978800692739}. Best is trial 14 with value: 0.8590909090909091.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:35:23,370]\u001b[0m Trial 15 finished with value: 0.8639610389610389 and parameters: {'eta': 0.9095725189484906, 'max_depth': 6, 'booster': 'gbtree', 'lambda': 1.0859424653734032e-05, 'alpha': 0.01836348773621795}. Best is trial 15 with value: 0.8639610389610389.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:35:35,158]\u001b[0m Trial 16 finished with value: 0.8623376623376623 and parameters: {'eta': 0.8393614007001113, 'max_depth': 6, 'booster': 'gbtree', 'lambda': 1.3628761886210529e-05, 'alpha': 0.6340091196368807}. Best is trial 15 with value: 0.8639610389610389.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:35:48,282]\u001b[0m Trial 17 finished with value: 0.8366883116883117 and parameters: {'eta': 0.09083359674467627, 'max_depth': 6, 'booster': 'gbtree', 'lambda': 0.0009153984728806333, 'alpha': 0.6569488391466882}. Best is trial 15 with value: 0.8639610389610389.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:35:59,073]\u001b[0m Trial 18 finished with value: 0.7961038961038961 and parameters: {'eta': 0.012876602532694275, 'max_depth': 5, 'booster': 'gbtree', 'lambda': 1.5202222384458097e-05, 'alpha': 0.03481398187867611}. Best is trial 15 with value: 0.8639610389610389.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:36:07,370]\u001b[0m Trial 19 finished with value: 0.826461038961039 and parameters: {'eta': 0.095559498434211, 'max_depth': 4, 'booster': 'gbtree', 'lambda': 0.0007821352447089865, 'alpha': 0.01607787403113063}. Best is trial 15 with value: 0.8639610389610389.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "xgb_bow_optimizer = xgb_optimizer(D_train_bow, D_val_bow, D_test_bow, \"XGB BOW Optimzer\")\n",
    "xgb_bow_optimizer.optimize(20)\n",
    "xgb_bow = xgb_bow_optimizer.get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "875a7392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-21 23:36:16,131]\u001b[0m A new study created in memory with name: XGB TF-IDF Optimzer\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:36:58,658]\u001b[0m Trial 0 finished with value: 0.8422077922077922 and parameters: {'eta': 0.356575162645744, 'max_depth': 2, 'booster': 'dart', 'lambda': 0.017245465493907514, 'alpha': 4.022314211971918e-07}. Best is trial 0 with value: 0.8422077922077922.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:37:42,671]\u001b[0m Trial 1 finished with value: 0.8542207792207792 and parameters: {'eta': 0.3701837019177327, 'max_depth': 6, 'booster': 'gbtree', 'lambda': 0.0011355814000466925, 'alpha': 0.00020136925614271174}. Best is trial 1 with value: 0.8542207792207792.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:38:23,333]\u001b[0m Trial 2 finished with value: 0.7853896103896104 and parameters: {'eta': 0.0020483243989624224, 'max_depth': 5, 'booster': 'gbtree', 'lambda': 0.0007138407093633312, 'alpha': 0.019510477146694363}. Best is trial 1 with value: 0.8542207792207792.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:38:55,661]\u001b[0m Trial 3 finished with value: 0.7881493506493507 and parameters: {'eta': 0.01479906877306922, 'max_depth': 4, 'booster': 'gbtree', 'lambda': 0.0004493309189566393, 'alpha': 1.1993981490447622e-07}. Best is trial 1 with value: 0.8542207792207792.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:39:14,668]\u001b[0m Trial 4 finished with value: 0.8423701298701298 and parameters: {'eta': 0.4322810415528501, 'max_depth': 2, 'booster': 'gbtree', 'lambda': 0.00014485550332581743, 'alpha': 9.514168800641288e-06}. Best is trial 1 with value: 0.8542207792207792.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:39:58,577]\u001b[0m Trial 5 finished with value: 0.8021103896103896 and parameters: {'eta': 0.07250414465736442, 'max_depth': 2, 'booster': 'dart', 'lambda': 0.0002651856448173145, 'alpha': 0.0005466604606570095}. Best is trial 1 with value: 0.8542207792207792.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:40:30,301]\u001b[0m Trial 6 finished with value: 0.7699675324675325 and parameters: {'eta': 0.00407560273353553, 'max_depth': 4, 'booster': 'gbtree', 'lambda': 6.9039261625257e-08, 'alpha': 2.8889192042875544e-07}. Best is trial 1 with value: 0.8542207792207792.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:41:20,627]\u001b[0m Trial 7 finished with value: 0.8517857142857143 and parameters: {'eta': 0.565555987752049, 'max_depth': 4, 'booster': 'dart', 'lambda': 0.001545371222993233, 'alpha': 3.0938760665137377e-06}. Best is trial 1 with value: 0.8542207792207792.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:42:06,326]\u001b[0m Trial 8 finished with value: 0.7678571428571429 and parameters: {'eta': 0.011815142174086506, 'max_depth': 3, 'booster': 'dart', 'lambda': 1.296177950706122e-07, 'alpha': 1.1072853056606327e-06}. Best is trial 1 with value: 0.8542207792207792.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:42:57,612]\u001b[0m Trial 9 finished with value: 0.8064935064935065 and parameters: {'eta': 0.01073247286550183, 'max_depth': 6, 'booster': 'gbtree', 'lambda': 2.050568015046804e-07, 'alpha': 0.0005730929118134707}. Best is trial 1 with value: 0.8542207792207792.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:43:48,450]\u001b[0m Trial 10 finished with value: 0.8368506493506493 and parameters: {'eta': 0.09767113079830551, 'max_depth': 6, 'booster': 'gbtree', 'lambda': 0.5806730822154933, 'alpha': 0.43598834131579983}. Best is trial 1 with value: 0.8542207792207792.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:44:56,931]\u001b[0m Trial 11 finished with value: 0.8491883116883117 and parameters: {'eta': 0.978049538249959, 'max_depth': 5, 'booster': 'dart', 'lambda': 6.9539283131037405e-06, 'alpha': 2.9361424304067158e-05}. Best is trial 1 with value: 0.8542207792207792.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:45:59,051]\u001b[0m Trial 12 finished with value: 0.8462662337662338 and parameters: {'eta': 0.1710475110077569, 'max_depth': 5, 'booster': 'dart', 'lambda': 0.01456119288311362, 'alpha': 0.0018235360834076046}. Best is trial 1 with value: 0.8542207792207792.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:46:40,132]\u001b[0m Trial 13 finished with value: 0.8496753246753247 and parameters: {'eta': 0.9665394025667525, 'max_depth': 3, 'booster': 'dart', 'lambda': 1.2806237699150223e-05, 'alpha': 9.811252850872982e-06}. Best is trial 1 with value: 0.8542207792207792.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:47:02,684]\u001b[0m Trial 14 finished with value: 0.8027597402597403 and parameters: {'eta': 0.05049994595764305, 'max_depth': 3, 'booster': 'gbtree', 'lambda': 0.011195253395579056, 'alpha': 3.0297657274006533e-08}. Best is trial 1 with value: 0.8542207792207792.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:48:01,704]\u001b[0m Trial 15 finished with value: 0.8474025974025974 and parameters: {'eta': 0.19585411479705284, 'max_depth': 5, 'booster': 'dart', 'lambda': 0.6098043146862084, 'alpha': 0.04326033207529336}. Best is trial 1 with value: 0.8542207792207792.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:48:40,840]\u001b[0m Trial 16 finished with value: 0.8555194805194806 and parameters: {'eta': 0.3435806042975683, 'max_depth': 6, 'booster': 'gbtree', 'lambda': 0.00331870444670386, 'alpha': 8.864308486745386e-05}. Best is trial 16 with value: 0.8555194805194806.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:49:25,987]\u001b[0m Trial 17 finished with value: 0.8178571428571428 and parameters: {'eta': 0.033406581852016094, 'max_depth': 6, 'booster': 'gbtree', 'lambda': 9.899716045992042e-06, 'alpha': 9.235124336862954e-05}. Best is trial 16 with value: 0.8555194805194806.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:50:07,359]\u001b[0m Trial 18 finished with value: 0.8472402597402597 and parameters: {'eta': 0.14575105110568914, 'max_depth': 6, 'booster': 'gbtree', 'lambda': 0.08031033037992596, 'alpha': 0.007210913258632832}. Best is trial 16 with value: 0.8555194805194806.\u001b[0m\n",
      "\u001b[32m[I 2022-04-21 23:50:45,989]\u001b[0m Trial 19 finished with value: 0.8561688311688311 and parameters: {'eta': 0.2987187695687918, 'max_depth': 6, 'booster': 'gbtree', 'lambda': 1.0018193997949487e-06, 'alpha': 0.00017601637090556026}. Best is trial 19 with value: 0.8561688311688311.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "xgb_tfidf_optimizer = xgb_optimizer(D_train_tfidf, D_val_tfidf, D_test_tfidf, \"XGB TF-IDF Optimzer\")\n",
    "xgb_tfidf_optimizer.optimize(20)\n",
    "xgb_tfidf = xgb_tfidf_optimizer.get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "092f1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let models train fully \n",
    "bow_param = {'eta': 0.395725189484906, \n",
    "        'max_depth': 8,  \n",
    "        'booster': \"gbtree\",\n",
    "        'lambda': 1.0859424653734032e-05,\n",
    "        'alpha': 0.01836348773621795,\n",
    "        'objective': 'multi:softprob',  \n",
    "        'eval_metric': 'mlogloss',\n",
    "        'num_class': 6} \n",
    "\n",
    "xgb_bow = xgb.train(bow_param, D_train_bow, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4dd93db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_param = {'eta': 0.2987187695687918, \n",
    "        'max_depth': 8,  \n",
    "        'booster': \"gbtree\",\n",
    "        'lambda': 1.0018193997949487e-06,\n",
    "        'alpha': 0.00017601637090556026,\n",
    "        'objective': 'multi:softprob',  \n",
    "        'eval_metric': 'mlogloss',\n",
    "        'num_class': 6} \n",
    "\n",
    "xgb_tfidf = xgb.train(tfidf_param, D_train_tfidf, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "84b506fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = xgb_bow.predict(D_test_bow)\n",
    "y_pred_xgb_bow = np.argmax(y_pred_prob, axis = 1)\n",
    "\n",
    "y_pred_prob = xgb_tfidf.predict(D_test_tfidf)\n",
    "y_pred_xgb_tfidf = np.argmax(y_pred_prob, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1d520fed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW Embedding Classification Report\n",
      "Model Accuracy: 0.8644973288003885\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.68      0.64       899\n",
      "           1       0.93      0.87      0.90      1083\n",
      "           2       0.96      0.94      0.95      1183\n",
      "           3       0.62      0.63      0.62       744\n",
      "           4       0.98      0.97      0.97      1177\n",
      "           5       0.99      0.98      0.99      1091\n",
      "\n",
      "    accuracy                           0.86      6177\n",
      "   macro avg       0.85      0.84      0.85      6177\n",
      "weighted avg       0.87      0.86      0.87      6177\n",
      " \n",
      "\n",
      "TFIDF Embedding Classification Report\n",
      "Model Accuracy: 0.8559171118666019\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.66      0.62       899\n",
      "           1       0.93      0.85      0.89      1083\n",
      "           2       0.97      0.94      0.95      1183\n",
      "           3       0.58      0.61      0.60       744\n",
      "           4       0.98      0.97      0.97      1177\n",
      "           5       0.99      0.98      0.99      1091\n",
      "\n",
      "    accuracy                           0.86      6177\n",
      "   macro avg       0.84      0.83      0.84      6177\n",
      "weighted avg       0.87      0.86      0.86      6177\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"BOW Embedding Classification Report\")\n",
    "print(f\"Model Accuracy: {accuracy(y_test, y_pred_xgb_bow)}\")\n",
    "print(classification_report(y_test, y_pred_xgb_bow), \"\\n\")\n",
    "\n",
    "\n",
    "print(\"TFIDF Embedding Classification Report\")\n",
    "print(f\"Model Accuracy: {accuracy(y_test, y_pred_xgb_tfidf)}\")\n",
    "print(classification_report(y_test, y_pred_xgb_tfidf), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c4da68",
   "metadata": {},
   "source": [
    "# Probability Averaging Ensemble \n",
    "\n",
    "While the three models have similar accuracy they may perform differently class by class. With the probability averaging ensemble the three model probabilties per class are averaged before a classification is chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9fee555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrain svms to enable probability\n",
    "svm_bow = SVC(max_iter = 10000, C = 0.13422383454969025, probability = True,\n",
    "                kernel = \"linear\").fit(X_train_bow, y_train)\n",
    "\n",
    "svm_tfidf = SVC(max_iter = 10000, C = 0.9384186373383406, probability = True,\n",
    "                kernel = \"linear\").fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e774c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow ensemble \n",
    "avg_probs_bow = np.mean(np.array([\n",
    "        log_reg_bow.predict_proba(X_test_bow),\n",
    "        xgb_bow.predict(D_test_bow),\n",
    "        svm_bow.predict_proba(X_test_bow)]), axis = 0)\n",
    "\n",
    "y_pred_ens_bow = np.argmax(avg_probs_bow, axis = 1)\n",
    "\n",
    "#tfidf ensemble\n",
    "avg_probs_tfidf = np.mean(np.array([\n",
    "        log_reg_tfidf.predict_proba(X_test_tfidf),\n",
    "        xgb_tfidf.predict(D_test_tfidf),\n",
    "        svm_tfidf.predict_proba(X_test_tfidf)]), axis = 0)\n",
    "\n",
    "y_pred_ens_tfidf = np.argmax(avg_probs_tfidf, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "26489ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW Embedding Classification Report\n",
      "Model Accuracy: 0.8623927472883277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.66      0.62       899\n",
      "           1       0.92      0.88      0.90      1083\n",
      "           2       0.95      0.95      0.95      1183\n",
      "           3       0.63      0.61      0.62       744\n",
      "           4       0.98      0.97      0.97      1177\n",
      "           5       0.99      0.98      0.98      1091\n",
      "\n",
      "    accuracy                           0.86      6177\n",
      "   macro avg       0.84      0.84      0.84      6177\n",
      "weighted avg       0.87      0.86      0.86      6177\n",
      " \n",
      "\n",
      "TFIDF Embedding Classification Report\n",
      "Model Accuracy: 0.861583292860612\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.62      0.61       899\n",
      "           1       0.91      0.88      0.89      1083\n",
      "           2       0.95      0.96      0.95      1183\n",
      "           3       0.63      0.62      0.62       744\n",
      "           4       0.96      0.97      0.97      1177\n",
      "           5       0.98      0.98      0.98      1091\n",
      "\n",
      "    accuracy                           0.86      6177\n",
      "   macro avg       0.84      0.84      0.84      6177\n",
      "weighted avg       0.86      0.86      0.86      6177\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"BOW Embedding Classification Report\")\n",
    "print(f\"Model Accuracy: {accuracy(y_test, y_pred_ens_bow)}\")\n",
    "print(classification_report(y_test, y_pred_ens_bow), \"\\n\")\n",
    "\n",
    "\n",
    "print(\"TFIDF Embedding Classification Report\")\n",
    "print(f\"Model Accuracy: {accuracy(y_test, y_pred_ens_tfidf)}\")\n",
    "print(classification_report(y_test, y_pred_ens_tfidf), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3e9047a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW+LR\n",
      "f1:\t0.8313 accuracy:\t0.8527\n",
      "\n",
      "BOW+SVM\n",
      "f1:\t0.8345 accuracy:\t0.8540\n",
      "\n",
      "BOW+XGB\n",
      "f1:\t0.8454 accuracy:\t0.8645\n",
      "\n",
      "TF-IDF+LR\n",
      "f1:\t0.8242 accuracy:\t0.8473\n",
      "\n",
      "TF-IDF+SVM\n",
      "f1:\t0.8361 accuracy:\t0.8575\n",
      "\n",
      "TF-IDF+XGB\n",
      "f1:\t0.8360 accuracy:\t0.8559\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [\"BOW+LR\", \"BOW+SVM\", \"BOW+XGB\", \n",
    "          \"TF-IDF+LR\", \"TF-IDF+SVM\", \"TF-IDF+XGB\"]\n",
    "predictions = [y_pred_lr_bow, y_pred_svm_bow, y_pred_xgb_bow, \n",
    "               y_pred_lr_tfidf, y_pred_svm_tfidf, y_pred_xgb_tfidf]\n",
    "\n",
    "for model, y_hat in zip(models, predictions): \n",
    "    print(model)\n",
    "    f1_macro = f1(y_test, y_hat, average=\"macro\")\n",
    "    print(f\"f1:\\t{f1_macro:0.4f}\", f\"accuracy:\\t{accuracy(y_test, y_hat):0.4f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
