{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-7556f7e98ae4>:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"features\"] = train_input_ids.tolist()\n",
      "<ipython-input-1-7556f7e98ae4>:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"masks\"] = train_attention_masks\n",
      "<ipython-input-1-7556f7e98ae4>:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"features\"] = test_input_ids.tolist()\n",
      "<ipython-input-1-7556f7e98ae4>:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"masks\"] = test_attention_masks\n",
      "<ipython-input-1-7556f7e98ae4>:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['not_cyberbullying'] = lst1\n",
      "<ipython-input-1-7556f7e98ae4>:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['gender'] = lst2\n",
      "<ipython-input-1-7556f7e98ae4>:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['religion'] = lst3\n",
      "<ipython-input-1-7556f7e98ae4>:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['other_cyberbullying'] = lst4\n",
      "<ipython-input-1-7556f7e98ae4>:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['age'] = lst5\n",
      "<ipython-input-1-7556f7e98ae4>:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['ethnicity'] = lst6\n",
      "<ipython-input-1-7556f7e98ae4>:137: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  train_masks = torch.tensor(train_masks, dtype=torch.long)\n",
      "<ipython-input-1-7556f7e98ae4>:138: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  valid_masks = torch.tensor(valid_masks, dtype=torch.long)\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/laurenwilkes/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Apr 17 17:18:05 2022\n",
    "\n",
    "@author: laurenwilkes\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import XLNetTokenizer, XLNetModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv('~/Downloads/long_tweets.csv')\n",
    "\n",
    "df['tweet_text']\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, XLNetTokenizer, XLNetModel, XLNetLMHeadModel, XLNetConfig\n",
    "from tensorflow import keras \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train, test= train_test_split(df, test_size=0.2)\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
    "\n",
    "train_text_list = train['tweet_text'].values\n",
    "test_text_list = test['tweet_text'].values\n",
    "\n",
    "\n",
    "def tokenize_inputs(text_list, tokenizer, num_embeddings=512):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text input into ids. Appends the appropriate special\n",
    "    characters to the end of the text to denote end of sentence. Truncate or pad\n",
    "    the appropriate sequence length.\n",
    "    \"\"\"\n",
    "    # tokenize the text, then truncate sequence to the desired length minus 2 for\n",
    "    # the 2 special characters\n",
    "    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t)[:num_embeddings-2], text_list))\n",
    "    # convert tokenized text into numeric ids for the appropriate LM\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    # append special token \"<s>\" and </s> to end of sentence\n",
    "    input_ids = [tokenizer.build_inputs_with_special_tokens(x) for x in input_ids]\n",
    "    # pad sequences\n",
    "    input_ids = pad_sequences(input_ids, maxlen=num_embeddings, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    return input_ids\n",
    "\n",
    "def create_attn_masks(input_ids):\n",
    "    \"\"\"\n",
    "    Create attention masks to tell model whether attention should be applied to\n",
    "    the input id tokens. Do not want to perform attention on padding tokens.\n",
    "    \"\"\"\n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "\n",
    "    # Create a mask of 1s for each token followed by 0s for padding\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "    return attention_masks\n",
    "\n",
    "train_input_ids = tokenize_inputs(train_text_list, tokenizer, num_embeddings=250)\n",
    "test_input_ids = tokenize_inputs(test_text_list, tokenizer, num_embeddings=250)\n",
    "\n",
    "train_attention_masks = create_attn_masks(train_input_ids)\n",
    "test_attention_masks = create_attn_masks(test_input_ids)\n",
    "\n",
    "train[\"features\"] = train_input_ids.tolist()\n",
    "train[\"masks\"] = train_attention_masks\n",
    "\n",
    "test[\"features\"] = test_input_ids.tolist()\n",
    "test[\"masks\"] = test_attention_masks\n",
    "\n",
    "\n",
    "#Adding the right shape to the data frame\n",
    "lst1 = [0] * 548\n",
    "lst2 = [0] * 548\n",
    "lst3 = [0] * 548\n",
    "lst4 = [0] * 548\n",
    "lst5 = [0] * 548\n",
    "lst6 = [0] * 548\n",
    "i=0\n",
    "for x in train['cyberbullying_type']:\n",
    "    if x == 'not_cyberbullying':\n",
    "        lst1[i] = 1\n",
    "    if x == 'gender':\n",
    "        lst2[i] = 1\n",
    "    if x == 'religion':\n",
    "        lst3[i] = 1\n",
    "    if x == 'other_cyberbullying':\n",
    "        lst4[i] = 1\n",
    "    if x == 'age':\n",
    "        lst5[i] = 1\n",
    "    if x == 'ethnicity':\n",
    "        lst6[i] = 1\n",
    "    i+=1\n",
    "    \n",
    "train['not_cyberbullying'] = lst1\n",
    "train['gender'] = lst2\n",
    "train['religion'] = lst3\n",
    "train['other_cyberbullying'] = lst4\n",
    "train['age'] = lst5\n",
    "train['ethnicity'] = lst6\n",
    "\n",
    "train, valid = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = train[\"features\"].values.tolist()\n",
    "X_valid = valid[\"features\"].values.tolist()\n",
    "\n",
    "train_masks = train[\"masks\"].values.tolist()\n",
    "valid_masks = valid[\"masks\"].values.tolist()\n",
    "\n",
    "\n",
    "label_cols = ['not_cyberbullying', 'gender', 'religion', 'other_cyberbullying','age', 'ethnicity']\n",
    "Y_train = train[label_cols].values.tolist()\n",
    "Y_valid = valid[label_cols].values.tolist()\n",
    "\n",
    "# AM I GOING TO END UP NEEDING MASKS AND SHIT\n",
    "\n",
    "X_train = torch.tensor(X_train)\n",
    "X_valid = torch.tensor(X_valid)\n",
    "\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
    "Y_valid = torch.tensor(Y_valid, dtype=torch.float32)\n",
    "\n",
    "train_masks = torch.tensor(train_masks, dtype=torch.long)\n",
    "valid_masks = torch.tensor(valid_masks, dtype=torch.long)\n",
    "\n",
    "# Select a batch size for training\n",
    "batch_size = 16\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on \n",
    "# memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(X_train, train_masks, Y_train)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,\\\n",
    "                              sampler=train_sampler,\\\n",
    "                              batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(X_valid, valid_masks, Y_valid)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,\\\n",
    "                                   sampler=validation_sampler,\\\n",
    "                                   batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(model, num_epochs,\\\n",
    "          optimizer,\\\n",
    "          train_dataloader, valid_dataloader,\\\n",
    "          model_save_path,\\\n",
    "          train_loss_set=[], valid_loss_set = [],\\\n",
    "          lowest_eval_loss=None, start_epoch=0,\\\n",
    "          device=\"cpu\"\n",
    "          ):\n",
    "  \"\"\"\n",
    "  Train the model and save the model with the lowest validation loss\n",
    "  \"\"\"\n",
    "\n",
    "  model.to(device)\n",
    "\n",
    "  # trange is a tqdm wrapper around the normal python range\n",
    "  for i in trange(num_epochs, desc=\"Epoch\"):\n",
    "    # if continue training from saved model\n",
    "    actual_epoch = start_epoch + i\n",
    "\n",
    "    # Training\n",
    "\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "\n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    num_train_samples = 0\n",
    "\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "      # Add batch to GPU\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      # Unpack the inputs from our dataloader\n",
    "      b_input_ids, b_input_mask, b_labels = batch\n",
    "      # Clear out the gradients (by default they accumulate)\n",
    "      optimizer.zero_grad()\n",
    "      # Forward pass\n",
    "      loss = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "      # store train loss\n",
    "      tr_loss += loss.item()\n",
    "      num_train_samples += b_labels.size(0)\n",
    "      # Backward pass\n",
    "      loss.backward()\n",
    "      # Update parameters and take a step using the computed gradient\n",
    "      optimizer.step()\n",
    "      #scheduler.step()\n",
    "\n",
    "    # Update tracking variables\n",
    "    epoch_train_loss = tr_loss/num_train_samples\n",
    "    train_loss_set.append(epoch_train_loss)\n",
    "\n",
    "    print(\"Train loss: {}\".format(epoch_train_loss))\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss = 0\n",
    "    num_eval_samples = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in valid_dataloader:\n",
    "      # Add batch to GPU\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      # Unpack the inputs from our dataloader\n",
    "      b_input_ids, b_input_mask, b_labels = batch\n",
    "      # Telling the model not to compute or store gradients,\n",
    "      # saving memory and speeding up validation\n",
    "      with torch.no_grad():\n",
    "        # Forward pass, calculate validation loss\n",
    "        loss = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # store valid loss\n",
    "        eval_loss += loss.item()\n",
    "        num_eval_samples += b_labels.size(0)\n",
    "\n",
    "    epoch_eval_loss = eval_loss/num_eval_samples\n",
    "    valid_loss_set.append(epoch_eval_loss)\n",
    "\n",
    "    print(\"Valid loss: {}\".format(epoch_eval_loss))\n",
    "\n",
    "    if lowest_eval_loss == None:\n",
    "      lowest_eval_loss = epoch_eval_loss\n",
    "      # save model\n",
    "      save_model(model, model_save_path, actual_epoch,\\\n",
    "                 lowest_eval_loss, train_loss_set, valid_loss_set)\n",
    "    else:\n",
    "      if epoch_eval_loss < lowest_eval_loss:\n",
    "        lowest_eval_loss = epoch_eval_loss\n",
    "        # save model\n",
    "        save_model(model, model_save_path, actual_epoch,\\\n",
    "                   lowest_eval_loss, train_loss_set, valid_loss_set)\n",
    "    print(\"\\n\")\n",
    "\n",
    "  return model, train_loss_set, valid_loss_set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model, save_path, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist):\n",
    "  \"\"\"\n",
    "  Save the model to the path directory provided\n",
    "  \"\"\"\n",
    "  model_to_save = model.module if hasattr(model, 'module') else model\n",
    "  checkpoint = {'epochs': epochs, \\\n",
    "                'lowest_eval_loss': lowest_eval_loss,\\\n",
    "                'state_dict': model_to_save.state_dict(),\\\n",
    "                'train_loss_hist': train_loss_hist,\\\n",
    "                'valid_loss_hist': valid_loss_hist\n",
    "               }\n",
    "  torch.save(checkpoint, save_path)\n",
    "  print(\"Saving model at epoch {} with validation loss of {}\".format(epochs,\\\n",
    "                                                                     lowest_eval_loss))\n",
    "  return\n",
    "  \n",
    "def load_model(save_path):\n",
    "  \"\"\"\n",
    "  Load the model from the path directory provided\n",
    "  \"\"\"\n",
    "  checkpoint = torch.load(save_path)\n",
    "  model_state_dict = checkpoint['state_dict']\n",
    "  model = XLNetForMultiLabelSequenceClassification(num_labels=model_state_dict[\"classifier.weight\"].size()[0])\n",
    "  model.load_state_dict(model_state_dict)\n",
    "\n",
    "  epochs = checkpoint[\"epochs\"]\n",
    "  lowest_eval_loss = checkpoint[\"lowest_eval_loss\"]\n",
    "  train_loss_hist = checkpoint[\"train_loss_hist\"]\n",
    "  valid_loss_hist = checkpoint[\"valid_loss_hist\"]\n",
    "  \n",
    "  return model, epochs, lowest_eval_loss, train_loss_hist, valid_loss_hist\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "#config = XLNetConfig()\n",
    "        \n",
    "class XLNetForMultiLabelSequenceClassification(torch.nn.Module):\n",
    "  \n",
    "  def __init__(self, num_labels=2):\n",
    "    super(XLNetForMultiLabelSequenceClassification, self).__init__()\n",
    "    self.num_labels = num_labels\n",
    "    self.xlnet = XLNetModel.from_pretrained('xlnet-base-cased')\n",
    "    self.classifier = torch.nn.Linear(768, num_labels)\n",
    "\n",
    "    torch.nn.init.xavier_normal_(self.classifier.weight)\n",
    "\n",
    "  def forward(self, input_ids, token_type_ids=None,\\\n",
    "              attention_mask=None, labels=None):\n",
    "    # last hidden layer\n",
    "    last_hidden_state = self.xlnet(input_ids=input_ids,\\\n",
    "                                   attention_mask=attention_mask,\\\n",
    "                                   token_type_ids=token_type_ids)\n",
    "    # pool the outputs into a mean vector\n",
    "    mean_last_hidden_state = self.pool_hidden_state(last_hidden_state)\n",
    "    logits = self.classifier(mean_last_hidden_state)\n",
    "        \n",
    "    if labels is not None:\n",
    "      loss_fct = BCEWithLogitsLoss()\n",
    "      loss = loss_fct(logits.view(-1, self.num_labels),\\\n",
    "                      labels.view(-1, self.num_labels))\n",
    "      return loss\n",
    "    else:\n",
    "      return logits\n",
    "    \n",
    "  def freeze_xlnet_decoder(self):\n",
    "    \"\"\"\n",
    "    Freeze XLNet weight parameters. They will not be updated during training.\n",
    "    \"\"\"\n",
    "    for param in self.xlnet.parameters():\n",
    "      param.requires_grad = False\n",
    "    \n",
    "  def unfreeze_xlnet_decoder(self):\n",
    "    \"\"\"\n",
    "    Unfreeze XLNet weight parameters. They will be updated during training.\n",
    "    \"\"\"\n",
    "    for param in self.xlnet.parameters():\n",
    "      param.requires_grad = True\n",
    "    \n",
    "  def pool_hidden_state(self, last_hidden_state):\n",
    "    \"\"\"\n",
    "    Pool the output vectors into a single mean vector \n",
    "    \"\"\"\n",
    "    last_hidden_state = last_hidden_state[0]\n",
    "    mean_last_hidden_state = torch.mean(last_hidden_state, 1)\n",
    "    return mean_last_hidden_state\n",
    "    \n",
    "model = XLNetForMultiLabelSequenceClassification(num_labels=len(Y_train[0]))\n",
    "#model = torch.nn.DataParallel(model)\n",
    "#model.cuda()\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01, correct_bias=False)\n",
    "#scheduler = WarmupLinearSchedule(optimizer, warmup_steps=num_warmup_steps, t_total=num_total_steps)  # PyTorch scheduler\n",
    "\n",
    "num_epochs=3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c pytorch torchvision cudatoolkit=9.0 pytorch\n",
    "#file = open(\"Downloads/xlnet_toxic.bin\", \"wb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0033424626057754914\n",
      "Valid loss: 0.0008652640591290864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|███▎      | 1/3 [28:05<56:10, 1685.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at epoch 0 with validation loss of 0.0008652640591290864\n",
      "\n",
      "\n",
      "Train loss: 0.0013551976716201157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|██████▋   | 2/3 [51:46<26:46, 1606.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.0010873947710603137\n",
      "\n",
      "\n",
      "Train loss: 0.0007083566851257963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 3/3 [1:13:57<00:00, 1479.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.0011569286604009737\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_save_path = output_model_file = os.path.join(\"Downloads/xlnet_toxic.bin\")\n",
    "model, train_loss_set, valid_loss_set = train(model=model,\\\n",
    "                                              num_epochs=num_epochs,\\\n",
    "                                              optimizer=optimizer,\\\n",
    "                                              train_dataloader=train_dataloader,\\\n",
    "                                              valid_dataloader=validation_dataloader,\\\n",
    "                                              model_save_path=model_save_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_save_path = output_model_file = os.path.join(\"Downloads/xlnet_toxic.bin\")\n",
    "model, start_epoch, lowest_eval_loss, train_loss_hist, valid_loss_hist = load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0048184811247121455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|███▎      | 1/3 [24:58<49:56, 1498.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.001531270204577595\n",
      "\n",
      "\n",
      "Train loss: 0.002941035337655244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|██████▋   | 2/3 [47:38<24:16, 1456.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.0034160106019540267\n",
      "\n",
      "\n",
      "Train loss: 0.0017285100293354255\n",
      "Valid loss: 0.0002916694465304979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 3/3 [1:11:31<00:00, 1430.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at epoch 2 with validation loss of 0.0002916694465304979\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs=3\n",
    "model, train_loss_set, valid_loss_set = train(model=model,\\\n",
    "                                              num_epochs=num_epochs,\\\n",
    "                                              optimizer=optimizer,\\\n",
    "                                              train_dataloader=train_dataloader,\\\n",
    "                                              valid_dataloader=validation_dataloader,\\\n",
    "                                              model_save_path=model_save_path,\\\n",
    "                                              train_loss_set=train_loss_hist,\\\n",
    "                                              valid_loss_set=valid_loss_hist,\\\n",
    "                                              lowest_eval_loss=lowest_eval_loss,\\\n",
    "                                              start_epoch=start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, df, num_labels, device=\"cpu\", batch_size=32):\n",
    "  num_iter = math.ceil(df.shape[0]/batch_size)\n",
    "  \n",
    "  pred_probs = np.array([]).reshape(0, num_labels)\n",
    "  \n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  \n",
    "  for i in range(num_iter):\n",
    "    df_subset = df.iloc[i*batch_size:(i+1)*batch_size,:]\n",
    "    X = df_subset[\"features\"].values.tolist()\n",
    "    masks = df_subset[\"masks\"].values.tolist()\n",
    "    X = torch.tensor(X)\n",
    "    masks = torch.tensor(masks, dtype=torch.long)\n",
    "    X = X.to(device)\n",
    "    masks = masks.to(device)\n",
    "    with torch.no_grad():\n",
    "      logits = model(input_ids=X, attention_mask=masks)\n",
    "      logits = logits.sigmoid().detach().cpu().numpy()\n",
    "      pred_probs = np.vstack([pred_probs, logits])\n",
    "  \n",
    "  return pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-986738fbaa7e>:14: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  masks = torch.tensor(masks, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(label_cols)\n",
    "pred_probs = generate_predictions(model, test, num_labels, batch_size=32)\n",
    "pred_probs\n",
    "rounded_pred_probs = np.round(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-abe1de875829>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['not_cyberbullying'] = rounded_pred_probs[:,0]\n",
      "<ipython-input-20-abe1de875829>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['gender'] = rounded_pred_probs[:,1]\n",
      "<ipython-input-20-abe1de875829>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['religion'] = rounded_pred_probs[:,2]\n",
      "<ipython-input-20-abe1de875829>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['other_cyberbullying'] = rounded_pred_probs[:,3]\n",
      "<ipython-input-20-abe1de875829>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['age'] = rounded_pred_probs[:,4]\n",
      "<ipython-input-20-abe1de875829>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['ethnicity'] = rounded_pred_probs[:,5]\n"
     ]
    }
   ],
   "source": [
    "label_cols = ['not_cyberbullying', 'gender', 'religion', 'other_cyberbullying','age', 'ethnicity']\n",
    "\n",
    "test['not_cyberbullying'] = rounded_pred_probs[:,0]\n",
    "test['gender'] = rounded_pred_probs[:,1]\n",
    "test['religion'] = rounded_pred_probs[:,2]\n",
    "test['other_cyberbullying'] = rounded_pred_probs[:,3]\n",
    "test['age'] = rounded_pred_probs[:,4]\n",
    "test['ethnicity'] = rounded_pred_probs[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test['cyberbullying_type']\n",
    "y_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>lang</th>\n",
       "      <th>score</th>\n",
       "      <th>y</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>features</th>\n",
       "      <th>masks</th>\n",
       "      <th>not_cyberbullying</th>\n",
       "      <th>gender</th>\n",
       "      <th>religion</th>\n",
       "      <th>other_cyberbullying</th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>17365</td>\n",
       "      <td>If you're a Christian, &amp;amp;you're ok w/voting...</td>\n",
       "      <td>religion</td>\n",
       "      <td>en</td>\n",
       "      <td>0.714285</td>\n",
       "      <td>2</td>\n",
       "      <td>288</td>\n",
       "      <td>[108, 44, 26, 88, 24, 31747, 19, 1123, 1701, 9...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>43635</td>\n",
       "      <td>You thought this was a read....however the boo...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>5</td>\n",
       "      <td>321</td>\n",
       "      <td>[44, 449, 52, 30, 24, 828, 9, 9, 9, 9, 7336, 3...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>42528</td>\n",
       "      <td>“all this hatred” are u kidding... white ppl h...</td>\n",
       "      <td>ethnicity</td>\n",
       "      <td>en</td>\n",
       "      <td>0.857140</td>\n",
       "      <td>5</td>\n",
       "      <td>288</td>\n",
       "      <td>[221, 2225, 52, 13184, 407, 41, 17, 660, 19880...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>14935</td>\n",
       "      <td>You are proud to hindu but you are not even sa...</td>\n",
       "      <td>religion</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>2</td>\n",
       "      <td>286</td>\n",
       "      <td>[44, 41, 4298, 22, 20187, 660, 57, 44, 41, 50,...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6999</td>\n",
       "      <td>You can either choose to host a fascist who is...</td>\n",
       "      <td>gender</td>\n",
       "      <td>en</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>1</td>\n",
       "      <td>284</td>\n",
       "      <td>[44, 64, 725, 1573, 22, 2057, 24, 17, 23476, 6...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                         tweet_text  \\\n",
       "233       17365  If you're a Christian, &amp;you're ok w/voting...   \n",
       "678       43635  You thought this was a read....however the boo...   \n",
       "661       42528  “all this hatred” are u kidding... white ppl h...   \n",
       "135       14935  You are proud to hindu but you are not even sa...   \n",
       "8          6999  You can either choose to host a fascist who is...   \n",
       "\n",
       "    cyberbullying_type lang     score  y  tweet_length  \\\n",
       "233           religion   en  0.714285  2           288   \n",
       "678          ethnicity   en  0.999995  5           321   \n",
       "661          ethnicity   en  0.857140  5           288   \n",
       "135           religion   en  0.999997  2           286   \n",
       "8               gender   en  0.999997  1           284   \n",
       "\n",
       "                                              features  \\\n",
       "233  [108, 44, 26, 88, 24, 31747, 19, 1123, 1701, 9...   \n",
       "678  [44, 449, 52, 30, 24, 828, 9, 9, 9, 9, 7336, 3...   \n",
       "661  [221, 2225, 52, 13184, 407, 41, 17, 660, 19880...   \n",
       "135  [44, 41, 4298, 22, 20187, 660, 57, 44, 41, 50,...   \n",
       "8    [44, 64, 725, 1573, 22, 2057, 24, 17, 23476, 6...   \n",
       "\n",
       "                                                 masks  not_cyberbullying  \\\n",
       "233  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...                0.0   \n",
       "678  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...                0.0   \n",
       "661  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...                0.0   \n",
       "135  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...                0.0   \n",
       "8    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...                0.0   \n",
       "\n",
       "     gender  religion  other_cyberbullying  age  ethnicity  \n",
       "233     0.0       1.0                  0.0  0.0        0.0  \n",
       "678     0.0       0.0                  0.0  0.0        1.0  \n",
       "661     0.0       0.0                  0.0  0.0        1.0  \n",
       "135     0.0       1.0                  0.0  0.0        0.0  \n",
       "8       1.0       0.0                  0.0  0.0        0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in test.iterrows():\n",
    "    if row[1]['not_cyberbullying'] == 1.0:\n",
    "        y_pred.append('not_cyberbullying')\n",
    "    if row[1]['gender'] == 1.0:\n",
    "        y_pred.append('gender')\n",
    "    if row[1]['religion'] == 1.0:\n",
    "        y_pred.append('religion')\n",
    "    if row[1]['other_cyberbullying'] == 1.0:\n",
    "        y_pred.append('other_cyberbullying')\n",
    "    if row[1]['age'] == 1.0:\n",
    "        y_pred.append('age')\n",
    "    if row[1]['ethnicity'] == 1.0:\n",
    "        y_pred.append('ethnicity')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_true.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "  not_cyberbullying       0.00      0.00      0.00         0\n",
      "             gender       0.95      1.00      0.97        19\n",
      "           religion       1.00      1.00      1.00        65\n",
      "other_cyberbullying       1.00      1.00      1.00         1\n",
      "                age       1.00      1.00      1.00        36\n",
      "          ethnicity       1.00      0.94      0.97        16\n",
      "\n",
      "          micro avg       0.99      0.99      0.99       137\n",
      "          macro avg       0.83      0.82      0.82       137\n",
      "       weighted avg       0.99      0.99      0.99       137\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurenwilkes/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/laurenwilkes/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/laurenwilkes/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/laurenwilkes/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/laurenwilkes/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/laurenwilkes/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, labels=['not_cyberbullying', 'gender', 'religion', 'other_cyberbullying','age', 'ethnicity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
